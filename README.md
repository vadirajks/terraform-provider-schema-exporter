Terraform Provider Schema Exporter
==================================

Python scripts to parse the output of terraform providers schema -json into organized and human-readable .tf, .yaml, and .json file trees.

What is this?
-------------

The terraform providers schema -json command is a powerful way to get the _entire_ schema for one or more providers. However, its output is a single, massive JSON file (often 20MB+) that is impossible to navigate.

These scripts solve that problem. They read the giant schema.json file and split it into thousands of individual files, organized by provider and schema type (resource, data source, etc.).

This creates a human-readable "schema library" that you can browse, search, and use as a reference.

### Core Features

*   **Generates .tf Syntax:** Creates .tf files that look like Terraform code, making them easy to read.
    
*   **Sorts Arguments:** .tf files are sorted with **(Required)** arguments at the top of each block.
    
*   **Labels Blocks:** Nested blocks are labeled as **(Required)** or **(Optional)**.
    
*   **Cleans Descriptions:** All \\n and \\t characters are stripped from descriptions for clean YAML and JSON output.
    
*   **Handles All Schemas:** Correctly processes resource\_schemas, data\_source\_schemas, resource\_identity\_schemas, and all other schema types.
    

Project Structure
-----------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   .  ├── all.sh                  # <-- Main script to run everything  ├── extract_schema_json.py  #  ├── extract_schema_tf.py    # <-- Core Python scripts  ├── extract_schema_yaml.py  #  ├── main.tf                 # <-- Your provider definitions  ├── output/                 # <-- All generated files land here  │   ├── aws_schema_json/  │   ├── aws_schema_tf/  │   ├── aws_schema_yaml/  │   ├── azurerm_schema_json/  │   ├── ...  ├── README.md  ├── schema.json             # <-- Generated by 'all.sh'  ├── split_schema_json.sh    #  ├── split_schema_tf.sh      # <-- Helper scripts  └── split_schema_yaml.sh    #   `

How to Use
----------

### Prerequisites

1.  **Terraform:** Must be installed.
    
2.  **Python 3:** Must be installed.
    
3.  pip install pyyaml
    

### Step 1: Configure Providers

Edit the main.tf file to include all the providers you want to export. You do not need to specify a version.

**main.tf**

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   terraform {    required_providers {      google = {        source  = "hashicorp/google"      }      azurerm = {        source  = "hashicorp/azurerm"      }      aws = {        source  = "hashicorp/aws"      }    }  }   `

### Step 2: Configure Helper Scripts

The split\_\*.sh scripts tell the Python scripts which providers to extract from the main schema.json. If you added a new provider to main.tf (e.g., kubernetes), you must also add it to all three split\_\*.sh scripts.

**split\_schema\_tf.sh (Example)**

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   python3 extract_schema_tf.py \    --provider google \    --schema-json schema.json \    --base-dir ./output  python3 extract_schema_tf.py \    --provider aws \    --schema-json schema.json \    --base-dir ./output  python3 extract_schema_tf.py \    --provider azurerm \    --schema-json schema.json \    --base-dir ./output   `

### Step 3: Run the Exporter

The all.sh script automates the entire process.

1.  chmod +x all.sh
    
2.  ./all.sh
    

This script will:

1.  Run terraform init to download the providers.
    
2.  Run terraform providers schema -json > schema.json to create the master schema file.
    
3.  Execute all three split\_\*.sh scripts to generate the output directories.
    

### Step 4: Browse the Output

You can now browse the generated schemas in the output/ directory.

The tree structure will look like this:

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   output/  ├── aws_schema_tf  │   └── provider_schemas  │       └── registry_terraform_io_hashicorp_aws  │           ├── data_source_schemas  │           │   ├── aws_instance.tf  │           │   └── ...  │           ├── provider  │           │   └── aws.tf  │           ├── resource_identity_schemas  │           │   ├── aws_iam_role.tf  │           │   └── ...  │           └── resource_schemas  │               ├── aws_instance.tf  │               └── ...  ├── azurerm_schema_tf  │   └── ...  └── google_schema_tf      └── ...   `

Example Output
--------------

The generated .tf files are formatted for readability, with required arguments and blocks listed first.

**Example: output/aws\_schema\_tf/.../resource\_schemas/aws\_instance.tf**

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   resource "aws_instance" "name" {    // Required arguments    ami = string (Required)    instance_type = string (Required)    // Optional arguments    associate_public_ip_address = bool (Optional)    availability_zone = string (Optional, Computed)    cpu_core_count = number (Optional, Computed)    // ...    capacity_reservation_specification block "list" (Optional) {      // Optional arguments      capacity_reservation_preference = string (Optional)      capacity_reservation_target block "list" (Optional) {        // Optional arguments        capacity_reservation_id = string (Optional)        capacity_reservation_resource_group_arn = string (Optional)      }    }    ebs_block_device block "set" (Optional, Computed) {      // Required arguments      device_name = string (Required)      // ...    }    // ...    // Computed arguments    arn = string (Computed)    id = string (Computed)    primary_network_interface_id = string (Computed)    // ...  }   `
