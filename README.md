# Terraform Provider Schema Exporter

Python scripts to parse the output of `terraform providers schema -json` into organized and human-readable `.tf`, `.yaml`, and `.json` file trees.

## What is this?

The `terraform providers schema -json` command is a powerful way to get the *entire* schema for one or more providers. However, its output is a single, massive JSON file (often 20MB+) that is impossible to navigate. These scripts solve that problem. They read the giant `schema.json` file and split it into thousands of individual files, organized by provider and schema type (resource, data source, etc.).

This creates a human-readable "schema library" that you can browse, search, and use as a reference.

### Core Features

* **Generates .tf Syntax:** Creates `.tf` files that look like Terraform code, making them easy to read.

* **Sorts Arguments:** `.tf` files are sorted with **(Required)** arguments at the top of each block.

* **Labels Blocks:** Nested blocks are labeled as **(Required)** or **(Optional)**.

* **Cleans Descriptions:** All `\n` and `\t` characters are stripped from descriptions for clean YAML and JSON output.

* **Handles All Schemas:** Correctly processes `resource_schemas`, `data_source_schemas`, `resource_identity_schemas`, and all other schema types.

## Project Structure

```
.
├── all.sh                  # <-- Main script to run everything
├── extract_schema_json.py  #
├── extract_schema_tf.py    # <-- Core Python scripts to parse schema.json
├── extract_schema_tf.py    # <-- Core Python scripts
├── extract_schema_yaml.py  #
├── LICENSE
├── main.tf                 # <-- Your provider definitions
├── output/                 # <-- All generated files land here
│   ├── aws_schema_json/
│   ├── aws_schema_tf/
│   ├── aws_schema_yaml/
│   ├── azurerm_schema_json/
│   └── ...
├── README.md
├── schema.json             # <-- Generated by 'all.sh'
├── split_schema_json.sh    #
├── split_schema_tf.sh      # <-- Helper scripts
└── split_schema_yaml.sh    #
├── split-aws.sh            # 
├── split-azurerm.sh        # <-- Helper scripts (one per provider)
└── split-google.sh         #
```

## How to Use

### Prerequisites

1.  **Terraform:** Must be installed.
2.  **Python 3:** Must be installed.
3.  **PyYAML:** The only Python dependency.
    ```bash
    pip install pyyaml
    ```

### Step 1: Configure Providers

Edit the `main.tf` file to include all the providers you want to export. You do not need to specify a version.

**`main.tf`**
```hcl
terraform {
  required_providers {
    google = {
      source = "hashicorp/google"
    }
    azurerm = {
      source = "hashicorp/azurerm"
    }
    aws = {
      source = "hashicorp/aws"
    }
  }
}
```

### Step 2: Configure Helper Scripts

To add a new provider, you don't need to edit any existing files. Instead, you create a new helper script for that provider. This approach is more scalable and keeps the configuration for each provider separate.

For example, to add the `kubernetes` provider, you would create a new file named `split-kubernetes.sh`:

**`split-kubernetes.sh` (New File)**
```bash
#!/bin/bash
PROVIDER_NAME="kubernetes"
BASE_DIR="./output"
SCHEMA_JSON="schema.json"

python3 extract_schema_tf.py --provider "$PROVIDER_NAME" --schema-json "$SCHEMA_JSON" --base-dir "$BASE_DIR"
python3 extract_schema_yaml.py --provider "$PROVIDER_NAME" --schema-json "$SCHEMA_JSON" --base-dir "$BASE_DIR"
python3 extract_schema_json.py --provider "$PROVIDER_NAME" --schema-json "$SCHEMA_JSON" --base-dir "$BASE_DIR"
```

### Step 3: Run the Exporter

The `all.sh` script automates the entire process.

1.  Make the script executable:
    ```bash
    chmod +x all.sh
    ```
2.  Run the script:
    ```bash
    ./all.sh
    ```
This script will:

1.  Run `terraform init` to download the providers.
2.  Run `terraform providers schema -json > schema.json` to create the master schema file.
3.  Execute all three `split_*.sh` scripts to generate the output directories.

### Step 4: Browse the Output

You can now browse the generated schemas in the `output/` directory.
The tree structure will look like this:

```
output/
├── aws_schema_tf
│   └── provider_schemas
│       └── registry_terraform_io_hashicorp_aws
│           ├── data_source_schemas
│           │   ├── aws_instance.tf
│           │   └── ...
│           ├── provider
│           │   └── aws.tf
│           ├── resource_identity_schemas
│           │   ├── aws_iam_role.tf
│           │   └── ...
│           └── resource_schemas
│               ├── aws_instance.tf
│               └── ...
├── azurerm_schema_tf
│   └── ...
└── google_schema_tf
    └── ...
```

## Example Output

The generated `.tf` files are formatted for readability, with required arguments and blocks listed first.

**Example: `output/aws_schema_tf/.../resource_schemas/aws_instance.tf`**
```hcl
resource "aws_instance" "name" {
  // Required arguments
  ami           = string (Required)
  instance_type = string (Required)

  // Optional arguments
  associate_public_ip_address = bool (Optional)
  availability_zone           = string (Optional, Computed)
  cpu_core_count              = number (Optional, Computed)
  // ...

  capacity_reservation_specification block "list" (Optional) {
    // Optional arguments
    capacity_reservation_preference = string (Optional)

    capacity_reservation_target block "list" (Optional) {
      // Optional arguments
      capacity_reservation_id               = string (Optional)
      capacity_reservation_resource_group_arn = string (Optional)
    }
  }

  ebs_block_device block "set" (Optional, Computed) {
    // Required arguments
    device_name = string (Required)
    // ...
  }

  // ...

  // Computed arguments
  arn                          = string (Computed)
  id                           = string (Computed)
  primary_network_interface_id = string (Computed)
  // ...
}
```
